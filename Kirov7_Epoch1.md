# CFC Studio 共学 Epoch1 指引

---

# Kirov7

> kirov reporting

## 笔记证明

<!-- Content_START --> 
### 1.11
#### LangChain 框架

LangChain 主要提供了 6 大核心组件帮助我们更好的使用大语言模型，涵盖了 Models（模型）、Prompts（提示）、Indexes（索引）、Memory（记忆）、Chains（链）、Agents（代理），这些组件集成了数十种大语言模型、多样的知识库处理方法以及成熟的应用链、上百种可调用的工具箱，为用户提供了一个快速搭建和部署大语言模型智能应用程序的平台。
![LangChain](https://heap.crazyfay.com/uploads/1736601823.png)

#### Prompt 组件

大多数 LLM 应用程序都不会直接将用户输入传递给 LLM。通常，它们会将用户输入添加到一个更大的文本片段中，称为提示模板，该模板提供有关特定任务的附加上下文。

大多数 LLM 应用程序都不会直接将用户输入传递给 LLM。通常，它们会将用户输入添加到一个更大的文本片段中，称为提示模板，该模板提供有关特定任务的附加上下文。并且 Prompt 是所有 AI 应用交互的起点，以下是 LangChain 中一个最基础的聊天应用机器人的运行流程如下：
![LangChain](https://heap.crazyfay.com/uploads/1736601994.jpg)

为了适配不同的 LLM，LangChain 封装了 Prompt 组件，并且 Prompt 组件是高可移植性的，同一个 Prompt 可以支持各种 LLM，在切换 LLM 的时候，无需修改 Prompt。在 LangChain 中，Prompt 被分成了两大类：

- **Prompt Template**：将 Prompt 按照 template 进行一定格式化，针对 Prompt 进行变量处理以及提示词的组合。
- **Selectors**：根据不同条件去选择不同提示词，或者在不同情况下通过 Selector，选择不同示例去进一步提高 Prompt 支持能力。
    >  本质上 Selectors 只是 Prompt Template 的二次封装

对于 Prompt Template，在 LangChain 中，又涵盖了多个子组件，例如：角色提示模板、消息占位符、文本提示模板、聊天消息提示模板、提示、消息等，Prompt Template 的运行流程如下：


![Prompt](https://heap.crazyfay.com/uploads/1736601929.jpg)

不同 Prompt 组件功能的简介：

- **PromptTemplate**：用于创建文本消息提示模板，用于用于与大语言模型/文本生成模型进行交互。
- **ChatPromptTemplate**：用于创建聊天消息提示模板，一般用于与聊天模型进行交互。
- **MessagePlaceholder**：消息占位符，在聊天模型中对不确定是否需要的消息进行占位。
- **SystemMessagePromptTemplate**：用于创建系统消息提示模板，角色为系统。
- **HumanMessagePromptTemplate**：用于创建人类消息提示模板，角色为人类。
- **AIMessagePromptTemplate**：用于创建AI消息提示模板，角色为AI。
- **PipelinePromptTemplate**：用于创建管道消息，管道消息可以将提示模板作为变量进行快速复用。

Prompt 不同方法的功能简介：

- **partial**：用于格式化提示模板中的部分变量。
- **format**：传递变量数据，格式化提示模板为文本消息。
- **invoke**：传递变量数据，格式化提示模板为提示。
- **to_string**：将提示/消息提示列表转换成字符串。
- **to_messages**：用于将提示转换成消息列表。

Prompt 中重载的运算符：

- **+ 运算符**：在 Prompt 组件中，对 + 运算符使用 `__add__` 方法进行重写，所以几乎所有 Prompt 组件都可以使用 + 进行组装拼接。

#### Model 组件
Model 是 LangChain 的核心组件，但是 LangChain 本身不提供自己的 LLM，而是提供了一个标准接口，用于封装不同类型的 LLM 进行交互，其中 LangChain 为两种类型的模型提供接口和集成：

- **LLM**：使用纯文本作为输入和输出的大语言模型。
- **Chat Model**：使用聊天消息列表作为输入并返回聊天消息的聊天模型。

在 LangChain 中，无论是 LLM 亦或者 Chat Model 都可以接受 PromptValue/字符串/消息列表 作为参数，内部会根据模型的类型自动转换成字符串亦或者消息列表，屏蔽了不同模型的差异。

对于 Model 组件，LangChain 有一个模型总基类，并对基类进行了划分：
![Model](https://heap.crazyfay.com/uploads/1736602318.jpg)

调用大模型最常用的方法为：

- **invoke**：传递对应的文本提示/消息提示，大语言模型生成对应的内容。
- **batch**：invoke 的批量版本，可以一次性生成多个内容。
- **stream**：invoke 的流式输出版本，大语言模型每生成一个字符就返回一个字符。

基础聊天应用的运行流程更改成如下：
![Model](https://heap.crazyfay.com/uploads/1736602471.jpg)

### 1.12

#### LLM 实现记忆功能思路
大多数的 LLM 应用程序都会有一个会话接口，允许我们和 LLM 进行多轮对话，并有一定的上下文记忆功能。但实际上，模型本身时不会记忆任何上下文的，只能依靠用户本身的输入去产生输出。而实现这个记忆功能，就需要额外的模块去保存我们和模型对话的上下文信息，然后在下一次请求时，把所有的历史信息都输入给模型，让模型输出结果。

所以为 LLM 添加记忆其实非常简单，就是在 Prompt 中预留 `chat_history` 占位符，将 `Human/Ai` 的历史对话信息插入到占位符中，并且实时保存 `Human/Ai` 的对话信息，在每一次对话时插入到预留占位符即可完成最简单的记忆功能。


#### 常见记忆模式
基于在 Prompt 中插入记忆内容，可以划分成几种记忆模式，例如：缓冲记忆、缓冲窗口记忆、令牌缓冲记忆、摘要总结记忆、摘要缓冲混合记忆、实体记忆、向量存储库记忆等，不同的记忆模式有不同的适用场景。

##### 缓冲记忆
最基础的记忆模式，将所有 `Human/Ai` 生成的消息全部存储起来，每次需要使用时将保存的所有聊天消息列表传递到 Prompt 中，通过往用户的输入中添加历史对话信息/记忆，可以让 LLM 能理解之前的对话内容，而且这种记忆方式在上下文窗口限制内是无损的。
- **优点**：
    1. 无损记忆，用户输入什么内容都会被记忆；
    2. 实现方式简单，兼容性最好，所有大模型都支持。
- **缺点**：
    1. 直接将存储的所有内容给 LLM，因为大量信息意味着新输入中包含更多的 Token，导致响应时间变慢和成本增加。
    2. 当达到 LLM 的令牌数限制时，太长的对话无法被记住。
    3. 记忆内容不是无限的，对于上下文长度较小的模型来说，记忆内容会变得极短。


##### 缓冲窗口记忆
缓冲窗口记忆只保存最近的几次 `Human/Ai` 生成的消息，它基于 `缓冲记忆` 思想，并添加了一个窗口值 `k`，这意味着只保留一定数量的过去互动，然后“忘记”之前的互动。
- **优点**：
    1. 缓冲窗口记忆在限制使用的 Token 数量表现优异。
    2. 对小模型也比较友好，不提问比较远的关联内容，一般效果最佳。
    3. 实现方式简单，性能优异，所有大模型都支持。
- **缺点**：
    1. 缓冲窗口记忆不适合遥远的互动，会忘记之前的“互动”。
    2. 部分对话内容长度较大，容易超过 LLM 的上下文限制。

##### 令牌缓冲记忆
缓冲窗口记忆只保存限定次数 `Human/Ai` 生成的消息，它基于 `缓冲记忆` 思想，并添加了一个令牌数 `max_tokens`，当聊天历史超过令牌数时，会遗忘之前的互动。

- **优点**：
    1. 可以基于大语言模型的上下文长度限制分配记忆长度。
    2. 对小模型也比较友好，不提问比较远的关联内容，一般效果最佳。
    3. 实现方式简单，性能优异，所有大模型都支持。
- **缺点**：
    1. 令牌缓冲记忆不适合遥远的互动，会忘记之前的“互动”。

##### 摘要总结记忆
除了将消息传递给 LLM，还可以将消息进行总结，每次只传递总结的信息，而不是完整的消息。这种模式记忆对于较长的对话最有用，可以避免过度使用 Token，因为将过去的信息历史以原文的形式保留在提示中会占用太多的 Token。

- **优点**：
    1. 无论是长期还是短期的互动都可以记忆（模糊记忆）。
    2. 减少长对话中使用 Token 的数量，能记忆更多轮的对话信息。
    3. 长对话时效果明显，虽然最初使用 Token 数量较多，随着对话进行，摘要方法增长速度减慢，与常规缓冲内存模型相比具有优势。
- **缺点**：
    1. 虽然能同时记住近期和长远的互动内容，但是记忆的细节部分会丢失；
    2. 对于较短的对话可能会增加 Token 使用量。
    3. 对话历史的记忆完全依赖于中间摘要 LLM 的能力，需要为摘要 LLM 分配 Token，增加成本且未限制对话长度。

##### 摘要缓冲混合记忆
摘要缓冲混合记忆结合了 `摘要总结记忆` 与 `缓冲窗口记忆`，它旨在对对话进行摘要总结，同时保留最近互动中的原始内容，但不是简单地清除旧的交互，而是将它们编译成摘要并同时使用，并且使用标记长度而不是交互数量来确定何时清除交互。

- **优点**：
    1. 无论是长期还是短期的互动都可以记忆，长期为模糊记忆，短期为精准记忆。
    2. 减少长对话中使用 Token 的数量，能记忆更多轮的对话信息。
- **缺点**：
    1. 长期互动的内容仍然为模糊记忆。
    2. 总结摘要部分完全依赖于中间摘要 LLM 的能力，需要为摘要 LLM 分配 Token，增加成本且未限制对话长度。

##### 向量存储库记忆
将记忆存储在向量存储中，并在每次调用时查询前 K 个最匹配的文档。这类记忆模式能记住所有内容，在细节部分比摘要总结要强，但是比缓冲记忆弱，消耗 Token 方面相对平衡。

- **优点**：
    1. 拥有比摘要总结更强的细节，比缓冲记忆能记忆更多的内容，甚至无限长度的内容；
    2. 消耗的 Token 也相对平衡；
- **缺点**：
    1. 性能相比其他模式相对较差，需要额外的 Embedding + 向量数据库支持。
    2. 记忆效果受检索功能的影响，好的非常好，差的非常差。

### 1.18
#### LangChain Memory组件
Memory 组件的基类是 `BaseMemory`，封装了大量的基础方法，例如：`memory_variables`、`load_memory_variables`、`aload_memory_variables`、`save_context`、`asave_context`、`clear`、`aclear` 函数。

基于 `BaseMemory` 基类，衍生出了两个子类 `SimpleMemory` 和 `BaseChatMemory`，当 LLM 应用不需要记忆功能，又不想更换代码结构时，可以将记忆组件使用 `SimpleMemory` 组件进行代替，`SimpleMemory` 实现了记忆组件的相关方法，但是不存储任何记忆，可以在不修改代码结构的情况下替换记忆组件，实现无记忆功能。

而 `BaseChatMemory` 组件是 LangChain 中内置的其他记忆组件的基类，针对对话历史进行了特定的封装，以适用聊天模型对话的场合。

LangChain 记忆组件的流程图如下：
![Memory](https://heap.crazyfay.com/uploads/1737202701.jpg)

在 LangChain 的 `BaseChatMemory` 组件中，不同的属性与方法有不同的作用：

- **chat_memory**：用于管理记忆中的历史消息对话。
- **output_key**：定义 AI 内容输出键。
- **input_key**：定义 Human 内容输入键。
- **return_messages**：`load_memory_variables` 函数是否返回消息列表，默认为 False 代表返回字符串。
- **save_context**：存储上下文到记忆组件中（存储消息对话）。
- **load_memory_variables**：生成加载到链的记忆字典信息。
- **clear**：清除记忆中的对话消息历史。
在聊天机器人的运行流程中，添加 `BaseChatMemory` 组件后，整体流程变化如下：
![Memory](https://heap.crazyfay.com/uploads/1737202843.jpg)

##### 缓冲记忆组件
缓冲记忆组件是 LangChain 中最简单的记忆组件，绝大部分都不对数据结构和提取算法做任何处理，就是简单的原进原出，也是使用频率最高的记忆组件，在 LangChain 中封装了几种内置的缓冲记忆组件，涵盖：

1. **ConversationBufferMemory**：缓冲记忆，最简单，最数据结构和提取算法不做任何处理，将所有对话信息全部存储作为记忆。
![Memory](https://heap.crazyfay.com/uploads/1737203295.jpg)

2. **ConversationBufferWindowMemory**：缓冲窗口记忆，通过设定 k 值，只保留一定数量（2*k）的对话信息作为历史。
![Memory](https://heap.crazyfay.com/uploads/1737203324.jpg)

3. **ConversationTokenBufferMemory**：令牌缓冲记忆，通过设置最大标记数量（max_token_limits）来决定何时清除交互信息，当对话信息超过 max_token_limits时，抛弃旧对话信息。
![Memory](https://heap.crazyfay.com/uploads/1737203485.jpg)

4. **ConversationStringBufferMemory**：字符串缓冲记忆（早期 LangChain 封装的记忆组件），等同于 缓冲记忆，固定返回字符串。
![Memory](https://heap.crazyfay.com/uploads/1737203523.jpg)

##### 摘要记忆组件
在 LangChain 中使用缓冲记忆组件要不就保存所有信息（占用过多容量），要不就保留最近的记忆信息（丢失太多重要信息），那么有没有一种情况是既要又要呢？

所以折中方案就出现了——保留关键信息（重点记忆），移除冗余噪音（流水式信息）。

在 LangChain 中 摘要记忆组件 就是一种折中的方案，内置封装的 摘要记忆组件 有以下几种。

1. **ConversationSummaryMemory**，摘要总结记忆组件，将传递的历史对话记录总结成摘要进行保存（底层使用 LLM 大语言模型进行总结），使用时填充的记忆为 摘要，并非对话数据。这种策略融合了记忆质量和容量的考量，只保留最核心的语义信息，有效减少了冗余，同时质量更高。
![Memory](https://heap.crazyfay.com/uploads/1737203577.jpg)

2. **ConversationSummaryBufferMemory**，摘要缓冲混合记忆，在不超过 max_token_limit 的限制下，保存对话历史数据，对于超过的部分，进行信息的提取与总结（底层使用 LLM 大语言模型进行总结），兼顾了精确的短期记忆与模糊的长期记忆。
![Memory](https://heap.crazyfay.com/uploads/1737203638.jpg)

##### 实体记忆组件介绍
实体记忆指的是跟踪对话中提到的实体，并且在对话中记住关于特定实体的既定事实，它提取关于实体的信息（使用LLM），并随着时间的推移建立对该实体的知识（使用LLM），一般使用实体记忆来存储和查询对话中引用的各种信息，比如人物、地点、事件等。

在 LangChain 内部封装了一个实体记忆类 `ConversationEntityMemory`，这个类可以从对话历史中提取实体并生成描述（简单来讲，就是提取关键词+对应的描述），不过其预设的 Prompt 过于笨重，而且极度消耗 Token，并且对大模型的要求极高，所以实用度并不高。

### 1.19
#### 大语言模型幻觉

大语言模型在处理自然语言时，有时会出现幻觉，表现为回答不准确或前后不一致的问题。这些幻觉可以分为两类：

- **事实性幻觉**：指模型生成的内容与可验证的现实事实不一致。比如提问“第一个在月球上行走的人是谁？“，模型回复“Charles Lindbergh在1951年月球先驱任务中第一个登上月球”，而实际上，第一个登上月球的人是Neil Armstrong。而事实性幻觉又分为事实不一致（与现实世界信息相矛盾）和事实捏造（压根没有，无法根据现实信息验证）。
- **忠实性幻觉**：指模型生成的内容与用户的指令或上下文不一致。比如让模型总结今年10月的新闻，结果模型却在说2006年10月的事。忠实性幻觉也可以细分，分为指令不一致（输出偏离用户指令）、上下文不一致（输出与上下文信息不符）、逻辑不一致（推理步骤以及与最终答案之间不一致）三类。

致使大模型产生幻觉的原因可以划分成三大来源：`数据源`、`训练过程` 和 `推理`。

##### 数据源导致的幻觉
首先病从口入，大模型的粮食数据，是致使它产生幻觉的一大原因。这里面就包括 数据缺陷 和数据中捕获的事实知识的 利用率 低。

具体来说，数据缺陷分为错误信息和偏见（重复偏见、社会偏见），此外大模型也存在知识边界，所以存在领域知识缺陷和过时的事实知识。即便大模型吃掉了大量的数据，也会在利用时出现问题。

除此之外，大模型可能会过度依赖训练数据中的一些模式，如位置接近性、共现统计数据和相关文档计数，从而导致幻觉，比如：如果训练数据中频繁出现“加拿大”和“多伦多”，那么大模型可能会错误地将多伦多识别为加拿大的首都。

##### 训练过程导致的幻觉
在模型的预训练阶段（大模型学习通用表示并获取世界知识）、对齐阶段（微调大模型使其更好地与人类偏好一致）两个阶段产生的问题也会导致幻觉的发生。

- 预训练阶段可能会存在：

    - **架构缺陷**：基于前一个 token 预测下一个 token，这种单向建模阻碍了模型捕获复杂的上下文关系的能力；自注意力模块存在缺陷，随着 token 长度增加，不同位置的注意力被稀释。
    - **暴露偏差**：训练策略也有缺陷，模型推理时依赖于自己生成的 token 进行后续预测，模型生成的错误 token 会在整个后续 token 中产生级联错误。

- 对齐阶段可能会存在：

    - **能力错位**：大模型内在能力与标注数据中描述的功能之间可能存在错位。当对齐数据需求超出这些预定义的能力边界时，大模型会被训练来生成超出其自身知识边界的内容，从而放大幻觉的风险。
    - **信念错位**：基于 RLHF 等的微调，使大模型的输出更符合人类偏好，但有时模型会倾向于迎合人类偏好，从而牺牲信息真实性。

##### 推理导致的幻觉
大模型产生幻觉的第三个关键因素是推理，存在两个问题：

- **固有的抽样随机性**：在生成内容时根据概率随机生成。
- **不完美的解码表示**：上下文关注不足（过度关注相邻文本而忽视了源上下文）和 softmax 瓶颈（输出概率分布的表达能力受限）。


#### 幻觉的缓解方案

幻觉和创造/创新/涌现其实只有一线之隔，大模型如果没有幻觉，那就永远无法产生新内容。所以，从涌现/创新的角度来说，大模型的幻觉永远不会被解决，在某些场合下只可能被缓解。

大语言模型幻觉的缓解方案也有`数据`、`预训练`、`对齐`、`推理`相关的方案，与 LLM 应用层开发相关的主要在 **`数据`** 方面的处理。
##### 缓解数据相关幻觉

减少错误信息和偏见，最直观的方法是收集高质量的事实数据，并进行数据清理以消除偏见。对于大语言模型知识边界的问题，有两种流行方法。一种是知识编辑，直接编辑模型参数弥合知识差距。另一种通过 **检索增强生成（RAG）** 利用非参数知识源。

检索增强生成具体分为三种类型：`一次性检索`、`迭代检索`和`事后检索`。
![幻觉](https://heap.crazyfay.com/uploads/1737289249.png)
- **一次性检索**：将从单次检索中获得的外部知识直接预置到大模型的提示中；
- **迭代检索**：允许在整个生成过程中不断收集知识；
- **事后检索**：基于检索的修订来完善大模型输出。

#####  缓解预训练相关幻觉
- **改进模型架构**：使用双向自回归模型和注意力锐化技术，增强模型对上下文的理解。
- **优化训练目标**：通过引入事实性增强的训练方法和上下文预训练，提升模型的事实关联和逻辑一致性。
- **减少曝光偏差**：采用新的监督信号和解码策略，减少训练与推理过程中的幻觉。

#####  缓解对齐相关幻觉
- **减少能力错位**：通过改进人类偏好判断，确保模型生成内容在其知识范围内。
- **减少信念错位**：聚合人类偏好和调整模型内部激活，减少模型迎合行为，避免生成与模型自身认知相悖的内容。

#####  缓解推理相关幻觉
- **增强事实性解码**：动态调整解码策略，利用模型内部结构引导事实性回答。
- **增强忠实度解码**：通过上下文和逻辑一致性策略，确保模型输出与用户指令或上下文保持一致。
<!-- Content_END -->